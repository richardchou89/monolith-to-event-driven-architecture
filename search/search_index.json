{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>An event-driven architecture consists of several components:</p> <ol> <li>Producer: for producing events.</li> <li>Message Broker: a component between producer and consumer. It is used to receive and distribute events to different consumers. The advantage of using a broker is that components are decoupled, and events can be processed asynchronously.</li> <li>Consumer: for receiving and processing events.</li> </ol> <p></p> <p>There are some design patterns for event-driven architecture:</p>"},{"location":"#eventbridge-eventbridge-rules","title":"EventBridge + EventBridge Rules","text":""},{"location":"#sns-sqs","title":"SNS + SQS","text":""},{"location":"author/","title":"About the Author","text":"<p>Richard Chou is a software developer with a broad range of experience in software development. He enjoys sharing what he has learned. You can find more of his writings on his blog https://blog.richardchou.dev/</p>"},{"location":"backup/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"backup/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"backup/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"benefits/","title":"Benefits","text":"<p>Some of the benefits of using queues in your architecture:</p> <ol> <li>Process tasks asynchronously: offload time-consuming operations, like sending emails, processing images, and generating embeddings.</li> <li>Communication between services: decouple your services by passing messages through a central queue.</li> <li>Load Balancing: Distribute tasks evenly across multiple workers.</li> </ol>"},{"location":"cover_art/","title":"Cover Art","text":"<p>Taipei 101, Taiwan </p> <p>Photo by Louis Cheng on Unsplash</p>"},{"location":"goal/","title":"Our Goal","text":"<p>In this guide, I'm going to demonstrate how to convert part of your monolith to an event-driven architecture using SNS and SQS.</p>"},{"location":"goal/#logical-representation","title":"Logical representation","text":""},{"location":"goal/#physical-representation","title":"Physical representation","text":""},{"location":"ruby_sdk/","title":"Adding a Publisher","text":"<p>Suppose we want to send an email. We can use following code example to send a SNS message from Ruby:</p> <pre><code>gem 'aws-sdk-sns', '~&gt; 1'\n\nsns = Aws::SNS::Client.new(\n  region: Rails.application.credentials.dig(:aws, :region),\n  access_key_id: Rails.application.credentials.dig(:aws, :access_key_id),\n  secret_access_key: Rails.application.credentials.dig(:aws, :secret_access_key)\n)\n\nsns.publish({\n  topic_arn: Rails.application.credentials.dig(:aws, :topic_arn),\n  message: {\n    recipient: 'abc@gmail.com',\n    subject: 'Hello',\n    body: 'This is an email'\n  }.to_json,\n  message_attributes: {\n    action: {\n      data_type: \"String\",\n      string_value: \"send_email\"\n    }\n  }\n})\n</code></pre> <p>Some of the settings are stored in env variables so each environment (staging, production...) can have its own setting.</p>"},{"location":"ruby_sdk/#how-to-test-on-local","title":"How to test on local","text":""},{"location":"ruby_sdk/#method-1-localstack","title":"Method 1: LocalStack","text":"<p>Set up SNS and SQS using LocalStack. And start Lambda in watch mode.</p> <p>This method is more formal but a bit involved.</p> <p>For setting SNS and SQS please refer to the LocalStack document.</p> <p></p>"},{"location":"ruby_sdk/#method-2-invoke-lambda-with-test-events","title":"Method 2: Invoke Lambda with test events","text":"<p>The second way is a bit easier to setup. All we need to do is to prepare event.json.</p> <p>We already know the shape of the event:</p> <p><pre><code>message: {\n  recipient: 'abc@gmail.com',\n  subject: 'Hello',\n  body: 'This is an email'\n}.to_json,\n</code></pre> and the action of the event (send_email)</p> <p>All we need to do is to put the message in the body of <code>events/event.json</code>.</p> <p><pre><code>{\n  \"Records\": [\n    {\n      \"messageId\": \"1c3e3d9a-85e0-457f-bc7e-45d96b56c587\",\n      \"receiptHandle\": \"MessageReceiptHandle1\",\n      \"body\": \"{recipient:'abc@gmail.com',subject:'Hello',body:'This is an email'}\",\n      \"attributes\": {\n        \"ApproximateReceiveCount\": \"1\",\n        \"SentTimestamp\": \"1625112000000\",\n        \"SenderId\": \"AIDAEXAMPLE\",\n        \"ApproximateFirstReceiveTimestamp\": \"1625112000000\"\n      },\n      \"messageAttributes\": {\n        \"Attribute1\": {\n          \"stringValue\": \"Value1\",\n          \"dataType\": \"String\"\n        },\n        \"Attribute2\": {\n          \"stringValue\": \"123\",\n          \"dataType\": \"Number\"\n        }\n      },\n      \"md5OfBody\": \"1d4a3a7151586b95d25fbbd2f9d57e38\",\n      \"eventSource\": \"aws:sqs\",\n      \"eventSourceARN\": \"arn:aws:sqs:us-east-1:123456789012:MyQueue\",\n      \"awsRegion\": \"us-east-1\"\n    }\n  ]\n}\n</code></pre> Other attributes like messageId, receiptHandle, attributes, messageAttributes...are not important.</p> <p>Invoke Lambda with this event: <pre><code>sam local invoke -e events/event.json --env-vars env_vars.json\n</code></pre> And in Lambda we just need to focus on our business logic (sending email).</p> <pre><code>type Email struct {\n  recipient string `json:\"recipient\"`\n  subject   string `json:\"subject\"`\n  body      string `json:\"body\"`\n}\n\nfunc handler(ctx context.Context, event events.SQSEvent) error {\n  for _, record := range event.Records {\n    fmt.Printf(\"Processing message ID: %s, Message body: %s\\n\", record.MessageId, record.Body)\n\n    var email Email\n    err := json.Unmarshal([]byte(record.Body), &amp;email)\n    if err != nil {\n      fmt.Println(\"Error parsing JSON:\", err)\n      continue\n    }\n\n    send(email)\n</code></pre> <p></p>"},{"location":"sam/","title":"Adding Consumers","text":"<p>A consumer contains a SQS and a Lambda. Each consumer can be managed by a team.</p> <ol> <li> <p>Each team can define its own data queueing policy, failed destination, or dead letter queue.</p> </li> <li> <p>Each team can define its own provisioned concurrency and preserved concurrency for Lambda.</p> </li> </ol> <p>An example of SAM template for provisioning SQS and Lambda looks like this:</p> template.yaml<pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: &gt;\n  lambda-go\n\n  Sample SAM Template for lambda-go\n\n# More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst\nGlobals:\n  Function:\n    Timeout: 30\n    MemorySize: 128\n\nParameters: # (4)\n  DefaultVPCSubnetIDs:\n    Type: \"List&lt;AWS::EC2::Subnet::Id&gt;\"\n    Description: \"List of Subnet IDs for the Lambda function to access resources within a VPC\"\n\n  EBEC2SecurityGroupIDs:\n    Type: \"List&lt;AWS::EC2::SecurityGroup::Id&gt;\"\n    Description: \"List of Security Group IDs for the Lambda function to control network access\"\n\n  DBHost:\n    Type: String\n    Description: \"Database host endpoint (e.g., RDS endpoint or IP address)\"\n\n  DBUser:\n    Type: String\n    Description: \"Database username\"\n\n  DBPass:\n    Type: String\n    Description: \"Database password\"\n\n  DBName:\n    Type: String\n    Description: \"Database name\"\n\n  DBPort:\n    Type: String\n    Description: \"Database port\"\n\n  DBSSLMode:\n    Type: String\n    Description: \"Database SSL mode\"\n\n  SNSTopicArn:\n    Type: String\n    Description: ARN of the existing SNS topic\n\n  Env:\n    Type: String\n    Description: Env\n\nResources:\n  GoMicroservice1Queue:\n    Type: AWS::SQS::Queue # (5)\n    Properties:\n      QueueName: !Sub \"${Env}-email-queue\"\n\n  GoMicroservice1Subscription:\n    Type: AWS::SNS::Subscription\n    Properties:\n      Protocol: sqs\n      Endpoint: !GetAtt GoMicroservice1Queue.Arn # (3)\n      TopicArn: !Ref SNSTopicArn\n      RawMessageDelivery: true # (6)\n      FilterPolicy:\n        action: # (1)\n          - 'send_email'\n\n  QueuePolicy:\n    Type: AWS::SQS::QueuePolicy\n    Properties:\n      Queues:\n        - !Ref GoMicroservice1Queue\n      PolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Principal: \"*\"\n            Action: \"SQS:SendMessage\"\n            Resource: !GetAtt GoMicroservice1Queue.Arn\n            Condition:\n              ArnEquals:\n                aws:SourceArn: !Ref SNSTopicArn\n\n  GoMicroserviceFunction:\n    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction\n    Metadata:\n      BuildMethod: go1.x\n    Properties:\n      CodeUri: hello-world/\n      Handler: bootstrap\n      Runtime: provided.al2023\n      Events:\n        SQSTrigger:\n          Type: SQS # (2)\n          Properties:\n            Queue: !GetAtt GoMicroservice1Queue.Arn\n            BatchSize: 10  # Number of messages to process at once\n            FunctionResponseTypes:\n              - ReportBatchItemFailures # (7)\n      Architectures:\n        - x86_64\n      VpcConfig:\n        SecurityGroupIds: !Ref EBEC2SecurityGroupIDs\n        SubnetIds: !Ref DefaultVPCSubnetIDs\n      Environment:\n        Variables:\n          DB_HOST: !Ref DBHost       # Use the parameter here\n          DB_USER: !Ref DBUser\n          DB_PASS: !Ref DBPass\n          DB_NAME: !Ref DBName\n          DB_PORT: !Ref DBPort\n          DB_SSL_MODE: !Ref DBSSLMode\n          ENV: !Ref Env\n\nOutputs:\n  # ServerlessRestApi is an implicit API created out of Events key under Serverless::Function\n  # Find out more about other implicit resources you can reference within SAM\n  # https://github.com/awslabs/serverless-application-model/blob/master/docs/internals/generated_resources.rst#api\n  GoMicroserviceFunction:\n    Description: \"First Lambda Function ARN\"\n    Value: !GetAtt GoMicroserviceFunction.Arn\n  GoMicroserviceFunctionIamRole:\n    Description: \"Implicit IAM Role created for Hello World function\"\n    Value: !GetAtt GoMicroserviceFunctionRole.Arn\n</code></pre> <ol> <li> <p>SNS will pass events with <code>send_email</code> to this Queue. When there are multiple consumers, each consumer can decide which event they want to receive.</p> </li> <li> <p>SQS will pass events to Lambda for processing. A batch consists of 10 events.</p> </li> <li> <p>Subscribe the Queue to SNS</p> </li> <li> <p>Pass environment variables to Lambda.</p> </li> <li> <p>Define a SQS queue</p> </li> <li> <p>Enable raw message delivery. <code>Message</code> will get delivered by SNS: <pre><code>message: {\n  recipient: 'abc@gmail.com',\n  subject: 'Hello',\n  body: 'This is an email'\n}.to_json\n</code></pre> For Amazon SQS subscriptions with Raw Message Delivery enabled, a maximum of 10 message attributes can be sent. So the following attributes will be sent by SNS as well: <pre><code>message_attributes: {\n  action: {\n    data_type: \"String\",\n    string_value: \"send_email\"\n  }\n}\n</code></pre> Source</p> </li> <li> <p>Lambda will only return failed items to SQS. It doesn't need to reprocess the whole batch again. See below \"Things we can improve\" for more information.</p> </li> </ol> <p>Here's an example of Lambda:</p> main.go<pre><code>type Email struct {\n  recipient string `json:\"recipient\"`\n  subject   string `json:\"subject\"`\n  body      string `json:\"body\"`\n}\n\nfunc handler(ctx context.Context, event events.SQSEvent) error {\n  for _, record := range event.Records {\n    fmt.Printf(\"Processing message ID: %s, Message body: %s\\n\", record.MessageId, record.Body)\n\n    var email Email\n    err := json.Unmarshal([]byte(record.Body), &amp;email)\n    if err != nil {\n      fmt.Println(\"Error parsing JSON:\", err)\n      continue\n    }\n\n    send(email)\n</code></pre>"},{"location":"sam/#things-we-can-improve","title":"Things we can improve","text":""},{"location":"sam/#sam-connector","title":"SAM Connector","text":"<p>SAM Connector simplifies the connections between source and destination resources in SAM template. You don't need to be an expert in IAM policies to connect resources.</p> <p>For example, to connect Lambda to DynamoDB:</p> template.yml<pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n...\nResources:\n  MyFunction:\n    Type: AWS::Lambda::Function\n    Connectors:\n      MyConn:\n        Properties:\n          Destination:\n            Id: MyTable\n          Permissions: # (1)\n            - Read\n            - Write\n  MyTable:\n    Type: AWS::DynamoDB::Table\n</code></pre> <ol> <li>Only needs three lines to define read and write permissions</li> </ol> <p>SAM Connector supports a wide range of resources. Try yourself to see if you can simplify the SAM template above (SNS to SQS, SQS to Lambda).</p>"},{"location":"sam/#handling-partial-batch-failures-in-lambda","title":"Handling partial batch failures in Lambda","text":"<p>With this feature, when messages on an SQS queue fail to process, Lambda marks a batch of records in a message queue as partially successful and allows reprocessing of only the failed records. By processing information at a record-level instead of batch-level, AWS Lambda has removed the need of repetitive data transfer, increasing throughput and making Amazon SQS message queue processing more efficient.</p> <p>Here are examples of Lambda returning failed items in different languages.</p>"},{"location":"terraform/","title":"Adding a Message Broker","text":"<p>We will be using Terraform to provision SNS in staging and production environments.</p> <p>There are different ways of using Terraform, depending on your AWS setup:</p>"},{"location":"terraform/#single-aws-account-deployment","title":"Single AWS account deployment","text":"<p>If all your environments (staging, production...) are in one account, then when you provision resources, you have to avoid naming conflict. For example:</p> <p>Production: <pre><code>arn:aws:sns:us-east-1:123456789012:MyTopic-Prod\n</code></pre></p> <p>Staging: <pre><code>arn:aws:sns:us-east-1:123456789012:MyTopic-Staging\n</code></pre></p> <p>Here's an example of provisioning SNS:</p> production/main.tf<pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"dep-terraform-state\"\n    key            = \"production/terraform.tfstate\"\n    region         = \"ap-southeast-2\"\n    dynamodb_table = \"terraform-state-locking\"\n    encrypt        = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.75.1\"\n    }\n  }\n}\n\nmodule \"ap-prod\" {\n  source = \"../modules/resources\" # (1)\n\n  aws_region = \"ap-southeast-2\" # (2)\n  env        = \"prod\" # (3)\n}\n\nmodule \"na-prod\" {\n  source = \"../modules/resources\"\n\n  aws_region = \"us-east-1\"\n  env        = \"prod\"\n}\n</code></pre> <ol> <li>Resources are defined in modules for reusability</li> <li>Passing variables to modules</li> <li>Passing variables to modules</li> </ol> staging/main.tf<pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"dep-terraform-state\"\n    key            = \"staging/terraform.tfstate\"\n    region         = \"ap-southeast-2\"\n    dynamodb_table = \"terraform-state-locking\"\n    encrypt        = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.75.1\"\n    }\n  }\n}\n\nmodule \"ap-staging\" {\n  source = \"../modules/resources\" # (1)\n\n  aws_region = \"ap-southeast-2\" # (2)\n  env        = \"staging\" # (3)\n}\n</code></pre> <ol> <li>Resources are defined in modules for reusability</li> <li>Passing variables to modules</li> <li>Passing variables to modules</li> </ol> modules/resources/main.tf<pre><code>provider \"aws\" {\n  region = var.aws_region\n}\n\nresource \"aws_sns_topic\" \"dep_sns\" {\n  name     = \"dep-${var.aws_region}-${var.env}\" # (1)\n\n  tags = {\n    Region      = var.aws_region\n    Environment = var.env\n  }\n}\n</code></pre> <ol> <li>Use the combination of region and environment to avoid naming conflict</li> </ol> modules/resources/outputs.tf<pre><code>output \"sns_arn\" {\n  value = aws_sns_topic.dep_sns.arn # (1)\n}\n</code></pre> <ol> <li>Output SNS arn so it can be used by other resources (it is not being used in our example)</li> </ol> modules/resources/variables.tf<pre><code>variable \"aws_region\" {\n  description = \"AWS Region\"\n  type        = string\n  default     = \"ap-southeast-2\"\n}\n\nvariable \"env\" {\n  description = \"Environment\"\n  type        = string\n  default     = \"staging\"\n}\n</code></pre>"},{"location":"terraform/#multi-aws-accounts-deployment","title":"Multi-AWS accounts deployment","text":"<p>If your environments are in multiple AWS accounts (for example, staging is in account A, production is in account B), then in general you don't have to worry about naming conflict.</p> <p>A common design pattern is:</p> <ol> <li>A Devops account responsible for deployments in each account</li> <li>An IAM role in each account, for which can be assumed by Devops account to deploy resources. The IAM role has to have a trust relationship with the Devops account.</li> </ol> <p></p> <p>Trust relationship would look like below:</p> IAM role in staging and production account<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::{DevOpsAccountID}:root\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:TagSession\"\n            ],\n            \"Condition\": {}\n        }\n    ]\n}\n</code></pre> <p>A GitHub Actions to run Terraform and deploy SNS would look like below:</p> .github/workflows/production.yml<pre><code>name: Prod Deployment\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: prod # (1)\n    steps:\n      - name: Assume Role\n        run: |\n          export AWS_ACCESS_KEY_ID=\"${{ secrets.DEVOPS_ACCOUNT_ACCESS_KEY_ID }}\"\n          export AWS_SECRET_ACCESS_KEY=\"${{ secrets.DEVOPS_ACCOUNT_SECRET_ACCESS_KEY }}\"\n          export AWS_REGION=\"${{ vars.AWS_REGION }}\"\n          export AWS_ROLE_TO_ASSUME=\"${{ secrets.ROLE_TO_ASSUME_ARN }}\"\n\n          CREDENTIAL=$(aws sts assume-role \\\n            --duration-seconds 900 \\\n            --role-arn $AWS_ROLE_TO_ASSUME \\\n            --role-session-name SAMSession \\\n            --output text \\\n            --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken,Expiration]')\n\n          export AWS_ACCESS_KEY_ID=$(echo $CREDENTIAL | awk '{print $1}')\n          export AWS_SECRET_ACCESS_KEY=$(echo $CREDENTIAL | awk '{print $2}')\n          export AWS_SESSION_TOKEN=$(echo $CREDENTIAL | awk '{print $3}')\n          export SESSION_EXPIRATION=$(echo $CREDENTIAL | awk '{print $4}')\n\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Terraform init\n        run: terraform init\n\n      - name: Terraform validate\n        run: terraform validate\n\n      - name: Terraform plan\n        run: terraform plan\n\n      - name: Terraform Deploy\n        run: terraform apply -auto-approve\n</code></pre> <ol> <li>Use GitHub's environment feature. Each environment can define its own secrets. We can use this to define secrets for staging and secrets for production.</li> </ol>"},{"location":"tools/","title":"Tools","text":""},{"location":"tools/#aws-sdk-for-ruby","title":"AWS SDK for Ruby","text":"<p>We will be using Ruby SDK to send messages to SNS.</p>"},{"location":"tools/#terraform","title":"Terraform","text":"<p>We will be using Terraform to provision SNS for staging and production environments.</p>"},{"location":"tools/#aws-sam","title":"AWS SAM","text":"<p>We will be using AWS SAM to provision SQS and Lambda.</p> <p></p>"},{"location":"where_to_go_from_here/","title":"Where to go from here","text":"<p>Designing a scalable, fault-tolerant and high-performance system is no small feat. Here are a few tips:</p> <ol> <li> <p>Lambda reserved and provisioned concurrency.</p> </li> <li> <p>Lambda failed destinations, dead-letter queue, invisible window, reporting batch item failures.</p> </li> <li> <p>Does the ordering of messages matter?</p> </li> <li> <p>Do all events go to the same SNS queue, or different queues because they are high volume?</p> </li> <li> <p>Lambda has a maximum timeout of 15 minutes. Does your task take longer than that? Do you need a long-running solution like Docker to process your tasks?</p> </li> </ol>"}]}